{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d074204-19b0-42d3-9199-2df2b02aa87b",
   "metadata": {},
   "source": [
    "Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f56b14-86fc-4188-ad83-b460650a9107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    Feature1  Feature2\n",
      "0      10.0       1.0\n",
      "1      20.0       NaN\n",
      "2       NaN       3.0\n",
      "3      40.0       4.0\n",
      "4      50.0       5.0\n",
      "\n",
      "Data after Imputation:\n",
      "    Feature1  Feature2\n",
      "0      10.0      1.00\n",
      "1      20.0      3.25\n",
      "2      30.0      3.00\n",
      "3      40.0      4.00\n",
      "4      50.0      5.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample dataset with missing values\n",
    "data = {'Feature1': [10, 20, np.nan, 40, 50], \n",
    "        'Feature2': [1, np.nan, 3, 4, 5]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Data:\\n\", df)\n",
    "\n",
    "# Replace NaN with column mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nData after Imputation:\\n\", df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca15e9-1ba0-462f-bf83-1042bec4c99a",
   "metadata": {},
   "source": [
    "Standardizing Data\n",
    "Feature Scaling (Normalization & Standardization)\n",
    "Many ML models work better when data is scaled. Scikit-Learn provides:\n",
    "\n",
    "MinMaxScaler (Normalization) – Scales between 0 and 1.\n",
    "StandardScaler (Standardization) – Converts data to have mean=0 and standard deviation=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e064125-e6f1-4f7f-9f89-f08e80a84a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized Data:\n",
      " [[-1.15311332 -1.18321596]\n",
      " [-0.73379939 -0.50709255]\n",
      " [ 0.52414242  0.16903085]\n",
      " [ 1.36277029  1.52127766]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = [[10, 200], [15, 250], [30, 300], [40, 400]]\n",
    "df = pd.DataFrame(data, columns=['Feature1', 'Feature2'])\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "print(\"\\nStandardized Data:\\n\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72711e-be7f-4521-9c69-696d236dfa7b",
   "metadata": {},
   "source": [
    "Encoding Categorical Variables\n",
    "ML models don’t understand text, so categorical features must be converted to numbers.\n",
    "Scikit-Learn provides:\n",
    "\n",
    "Label Encoding (LabelEncoder) – Converts categories to 0, 1, 2, …\n",
    "One-Hot Encoding (OneHotEncoder) – Creates binary columns for each category.\n",
    "Converts Red, Blue, Green into separate binary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47cd1c6f-c98c-4fdf-ba64-755034930abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-Hot Encoded Data:\n",
      " [[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample categorical data\n",
    "data = [['Red'], ['Blue'], ['Green'], ['Blue'], ['Red']]\n",
    "df = pd.DataFrame(data, columns=['Color'])\n",
    "\n",
    "# Apply OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_data = encoder.fit_transform(df)\n",
    "\n",
    "print(\"\\nOne-Hot Encoded Data:\\n\", encoded_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61767519-144c-4de5-bed7-e108121c6596",
   "metadata": {},
   "source": [
    "Feature Selection (Removing Irrelevant Features)\n",
    "Too many features can cause overfitting. We can remove irrelevant ones using SelectKBest.\n",
    "\n",
    "Example: Selecting Top 2 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9987a27e-00cd-4b71-adcd-0236b0f25231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Features Shape: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Select the best 2 features\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(\"\\nSelected Features Shape:\", X_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50fe194-1242-4373-9db3-0f2f9455c4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
