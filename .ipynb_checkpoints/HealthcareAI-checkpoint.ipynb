{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "566ee394-8935-44b5-9b13-d6783431a932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83       147\n",
      "           1       0.27      0.06      0.09        53\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.50      0.50      0.46       200\n",
      "weighted avg       0.61      0.71      0.63       200\n",
      "\n",
      "âœ… Model saved at: .\\model.pkl\n",
      "âœ… Scaler saved at: .\\scaler.pkl\n",
      "âœ… Features saved at: .\\features.pkl\n"
     ]
    }
   ],
   "source": [
    "# import statements: These import necessary libraries and modules.\n",
    "# pandas (pd): Used for data manipulation and analysis, particularly for handling data in tabular form (DataFrames).\n",
    "# numpy (np): Used for numerical operations, often with arrays.\n",
    "# os: Used for interacting with the operating system (file paths, directories).\n",
    "# pickle: Used for serializing (saving) and deserializing (loading) Python objects like models.\n",
    "# train_test_split: A method from sklearn to split data into training and test sets.\n",
    "# RandomForestClassifier: A machine learning algorithm used to build a classification model.\n",
    "# StandardScaler: A preprocessing technique that scales features to have a mean of 0 and standard deviation of 1.\n",
    "# classification_report: Generates a report that shows precision, recall, and F1 score for a classification model.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define Paths: These lines define the paths for the dataset, saved model, and scaler.\n",
    "# BASE_DIR = \".\": Refers to the current directory.\n",
    "# DATA_PATH: Path to the CSV dataset containing patient data.\n",
    "# MODEL_PATH and SCALER_PATH: Paths where the trained model and scaler will be saved after training.\n",
    "# ðŸ”¹ Define Paths\n",
    "\n",
    "BASE_DIR = \".\"\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"healthcare_dataset.csv\")\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"model.pkl\")\n",
    "SCALER_PATH = os.path.join(BASE_DIR, \"scaler.pkl\")\n",
    "FEATURES_PATH = os.path.join(BASE_DIR, \"features.pkl\")  # To save feature names\n",
    "\n",
    "# File Existence Check: This checks if the dataset exists at the specified path. If it doesn't, it raises an error with a helpful message.\n",
    "# ðŸ”¹ Ensure Dataset Exists\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Please ensure the file is present in {BASE_DIR}.\")\n",
    "\n",
    "# Loading and Preprocessing:\n",
    "# pd.read_csv(DATA_PATH): Loads the dataset from the CSV file into a DataFrame.\n",
    "# df.dropna(): Drops any rows with missing (NaN) values from the DataFrame.\n",
    "\n",
    "# ðŸ”¹ Load Data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.dropna()\n",
    "\n",
    "# pd.get_dummies(df, columns=['Gender', 'Diagnosis', 'Insurance_Type']): Performs one-hot encoding on the \n",
    "# categorical columns (Gender, Diagnosis, Insurance_Type), creating binary columns for each category (e.g., Gender_Male, Diagnosis_COPD, etc.).\n",
    "# ðŸ”¹ Identify Categorical Columns\n",
    "categorical_columns = ['Gender', 'Diagnosis', 'Insurance_Type']  # Update if there are other categorical columns\n",
    "\n",
    "# ðŸ”¹ One-hot Encoding for Categorical Variables\n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "# Feature and Target Separation:\n",
    "# X: Contains all columns except Readmission and Patient_ID (features for training).\n",
    "# y: Contains the target column, Readmission, which we are trying to predict.\n",
    "# Splitting Data:\n",
    "# train_test_split: Splits the data into training and testing sets.\n",
    "# X_train and y_train: Training features and target.\n",
    "# X_test and y_test: Test features and target.\n",
    "# test_size=0.2: 20% of the data is used for testing, 80% for training.\n",
    "# random_state=42: Ensures reproducibility of the split.\n",
    "\n",
    "# ðŸ”¹ Split Features and Target\n",
    "X = df.drop(columns=['Readmission', 'Patient_ID'])\n",
    "y = df['Readmission']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling Features:\n",
    "# StandardScaler: Standardizes the features by removing the mean and scaling to unit variance.\n",
    "# scaler.fit_transform(X_train): Fits the scaler to the training data and transforms it (scales it).\n",
    "# scaler.transform(X_test): Transforms the test data using the already fitted scaler.\n",
    "\n",
    "# ðŸ”¹ Create and Save Scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit & transform training data\n",
    "X_test_scaled = scaler.transform(X_test)        # Only transform test data\n",
    "\n",
    "# Model Definition:\n",
    "# RandomForestClassifier: Initializes a random forest classifier model with 100 trees (n_estimators=100).\n",
    "# model.fit(X_train_scaled, y_train): Trains the model on the scaled training data (X_train_scaled) and corresponding target values (y_train).\n",
    "\n",
    "# ðŸ”¹ Train Model with Scaled Data\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Creating Directory: Ensures the directory for saving the model and scaler exists (if not, it creates it).\n",
    "# ðŸ”¹ Save Feature Names for Consistency\n",
    "FEATURES = X.columns.tolist()\n",
    "with open(FEATURES_PATH, \"wb\") as features_file:\n",
    "    pickle.dump(FEATURES, features_file)\n",
    "\n",
    "\n",
    "# ðŸ”¹ Ensure Directory Exists\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "# Saving the Model: Serializes and saves the trained model to model.pkl.\n",
    "# ðŸ”¹ Save Model & Scaler\n",
    "with open(MODEL_PATH, \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "# Saving the Scaler: Serializes and saves the fitted scaler to scaler.pkl.\n",
    "with open(SCALER_PATH, \"wb\") as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "# Making Predictions:\n",
    "# model.predict(X_test_scaled): Makes predictions using the trained model on the scaled test data.\n",
    "# Evaluation:\n",
    "# classification_report(y_test, y_pred): Generates a classification report showing precision, recall, F1 score, and accuracy for \n",
    "# the model's predictions compared to the true values (y_test).\n",
    "# ðŸ”¹ Evaluate Model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(\"Model Evaluation:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confirmation Messages: Prints messages confirming that the model and scaler were saved successfully.\n",
    "print(f\"âœ… Model saved at: {MODEL_PATH}\")\n",
    "print(f\"âœ… Scaler saved at: {SCALER_PATH}\")\n",
    "print(f\"âœ… Features saved at: {FEATURES_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db2b639-4072-4266-8faa-9ee3298c81c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
